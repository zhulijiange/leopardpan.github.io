---
layout: post
title: 'day83笔记-'
description: 爬虫
tag: 博客
---     
### 京东商品抓取案例
    1. 目标
      1. 商品名称
      2. 商品价格
      3. 评论数量
      4. 商家名称
    2. xpath匹配每个商品的节点对象
      //div[@id="J_goodsList"]//li
    3. 关于下一页
      下一页按钮(能点): class的值为pn-next
      下一页按钮(不能点): class的值为pn-next disabled

### 多线程爬虫
    1. 进程
      1. 系统中正在运行的1个应用程序
      2. 1个CPU核心1次只能执行1个进程, 其它进程都属于非运行状态
      3. N个CPU核心可同时执行N个任务
    2. 线程
      1. 进程中包含的执行单元, 1个进程可包含多个线程
      2. 线程使用所属进程空间(1次只能执行1个线程, 阻塞)
    3. GIL: 全局解释锁
      执行通行证, 仅此1个, 谁拿到了通行证谁执行, 否则等待
    4. 应用场景
      1. 多进程: 大量的密集计算
      2. 多线程: I/O操作密集
        爬虫: 网络I/O密集
        写文件: 本地磁盘I/O
    5. 百思不得姐多线程案例
      1. 网址: http://www.budejie.com/
      2. 目标: 段子内容
      3. xpath表达式
        //div[@class="j-r-list-c-desc"]/a/text()
      4. 知识点
        1. 队列(from queue import Queue)
          put()
          get()
          Queue.empty(): 判断队列是否为空
          Queue.join(): 如果队列为空, 执行其它的程序
        2. 线程(import threading)
          threading.Thread(target=...)
      5. 代码实现

### BeautifulSoup解析
    1. 定义: HTML或XML的解析器, 依赖于lxml
    2. 安装: python -m pip install beautifulsoup4
      导入: from bs4 import BeautifulSoup
    3. 使用流程
      1. 导模块: from bs4 import BeautifulSoup
      2. 创建解析对象
        soup = BeautifulSoup(html, 'lxml')
      3. 查找节点对象
        r_list = soup.find_all("div", attrs={"class": "test"})
    4. 示例代码
    5. BeautifulSoup支持的解析库
      1. lxml: soup = BeautifulSoup(html, 'lxml')
        速度快, 文档容错能力强
      2. html.parser: Python标准库
        速度一般, 容错能力一般
      3. xml
        速度快, 文档容错能力强
    6. 节点选择器
      1. 选择节点并获取内容
        节点对象, 节点名.string
    7. find_all(): 返回列表
      r_list = soup.find_all("节点名", attrs={"": ""})

### Scrapy框架
    1. 定义
      异步处理框架, 可配置和可扩展程度非常高, Python中使用最广泛的爬虫框架
    2. 安装(Ubuntu)
      1. 安装依赖库
        sudo apt-get install python3-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
      2. 安装Scrapy
        sudo pip3  install Scrapy
      3. 验证
        import scrapy

        1. scrapy startproject XXX
          sudo pip3 install pyasnl --upgrade

### Scrapy框架五大组件
    1. 引擎(Engine): 整个框架的核心
    2. 调度器(Scheduler): 接受从引擎发过来的URL, 入队列
    3. 下载器(Downloader): 获取网页源码, 返给爬虫程序
    4. 下载器中间件(Downloader Middlewares)
       蜘蛛中间件(Spider Middlewares)
    5. 项目管道(Item Pipeline): 数据处理

### Scrapy框架详细抓取流程
### 制作Scrapy爬虫项目的步骤
    1. 新建项目
      scrapy startproject 项目名
    2. 明确目标(items.py)
    3. 制作爬虫程序
      进入到spiders文件夹中, 执行:
      scrapy genspider 文件名 "域名"
    4. 处理数据(pipelines.py)
    5. 配置settings.py
    6. 运行爬虫程序
      scrapy crawl 爬虫名

### scrapy项目文件详解
    Baidu/
    ├── Baidu: 项目目录
    │   ├── __init__.py
    │   ├── items.py: 定义爬取的数据结构
    │   ├── middlewares.py: 下载器中间件和蜘蛛中间件
    │   ├── pipelines.py: 管道文件, 处理数据
    │   ├── settings.py: 项目全局配置
    │   └── spiders: 文件夹, 存放爬虫程序
    │       ├── baiduspider.py: 爬虫程序/spider
    │       └── __init__.py
    └── scrapy.cfg: 项目的基本配置文件

### 文件配置详情
### 项目: 抓取百度首页源码, 存到 百度.html中
    1. scrapy startproject Baidu
    2. cd Baidu/Baidu
    3. subl items.py(无操作)
    4. cd spiders
    5. scrapy genspider baidu "www.baidu.com"
    6. subl baidu.py
      # 爬虫名
      # 域名
      # start_urls

      def parse(self, response):
          with open("百度.html", 'w') as f:
              f.write(response.text)
    7. cd../
    8. subl settings.py
      1. 把robots改为False
      2. 添加User-Agent
        DEFAULT_REQUEST_HEADERS = {
          "User-Agent", "Mozilla/5.0",
          ... ...
        }
