---
layout: post
title: 'day78笔记-爬虫简介, 爬取数据, 爬虫请求, 项目部署'
description: 爬虫
tag: 博客
---  
### 网络爬虫
    1. 定义: 网络蜘蛛, 网络机器人, 抓取网络数据的程序
    2. 总结: 用Python程序模仿人去访问网站, 模仿的越像人越好
    3. 爬取数据的目的: 通过有效的大量数据分析市场走势, 公司决策

### 企业获取数据的方式
    1. 公司自有数据
    2. 第三方数据平台购买
      数据堂, 贵阳大数据交易所
    3. 爬虫爬取数据
      市场上没有或者价格太高, 利用爬虫程序爬取

### Python做爬虫优势
    请求模块, 解析模块丰富成熟, 强大的scrapy框架(基于多线程)
    PHP: 对多线程, 异步支持不太好
    JAVA: 代码笨重, 代码量很大
    C/C++: 代码成型很慢

### 爬虫的分类
    1. 通用网络爬虫(搜索引擎引用, 需要遵守robots协议)
      https://www.taobao.com/robots.txt
      1. 搜索引擎如何获取一个新网站的URL
        1. 网站主动向搜索引擎提供(百度站长平台)
        2. 和DNS服务商(万网)合作, 快速收录新网站

    2. 聚焦网络爬虫
      自己写的爬虫程序: 面向需求的爬虫

### 爬取数据的步骤
    1. 确定要爬取的URL地址
    2. 通过HTTP/HTTPS协议获取相应的HTML页面
    3. 提取HTML页面中有用的数据
      1. 所需数据, 保存
      2. 页面中有其它的URL, 继续第2步

### 爬虫开发环境 Anaconda | 编辑器 Spyder
    1. Spyder常用的快捷键
      1. 注释和取消注释: Ctrl + 1
      2. 保存: Ctrl + s
      3. 运行: F5
      4. 自动补全: Tab

### Chrome浏览器插件
    1. 安装步骤
      1. 扩展程序
      2. 开发者模式
      3. 添加扩展程序
    2. 插件介绍
      1. Xpath Helper: 网页数据解析插件
      2. JSON View: 查看json格式的数据(好看)
      3. Proxy SwitchOmega: 代理切换插件

### WEB
    1. HTTP/HTTPS
      1. HTTP: 80
      2. HTTPS: 443, HTTP的升级版, 加了一个安全套接层
    2. GET/POST
      1. GET: 查询参数都会在URL地址上显示出来
      2. POST: 查询参数和需要提交的数据是隐藏在form表单里的, 不会再URL上显示
    3. URL: 统一资源地位符
      https://item.jd.com:80/443 /11936238.html #detail
       协议     域名/IP地址  端口     访问资源的路径   锚点
    4. User-Agent
      记录用户的浏览器, 操作系统等, 为了让用户获取更好的HTML页面效果     

### 爬虫请求模块
    1. 版本
      1. python2: urllib2 urllib
      2. python3: urllib.request
    2. 常用方法
      1. urllib.request.urlopen("网址")
        1. 作用: 向网站发起请求, 并获取响应
          字节流 = response.read()
          字符串 = response.read().decode("utf-8")
          # encode() 字符串-->bytes
          # decode() bytes-->字符串
        2. 重构User-Agent
          1. 不支持重构: urlopen()
          2. 支持重构User-Agent
            urllib.request.Request(添加User-Agent)
      2. urllib.request.Request()
        User-Agent是爬虫和反爬虫斗争的第1步, 发送请求必须带Users-Agent
        1. 使用流程
          1. 创建请求对象: 利用Request()方法
          2. 获取响应对象: 利用urlopen(请求对象)
          3. 获取响应内容: html = res.read().decode("utf-8")
        2. 响应对象res的方法
          1. read(): 读取响应内容(字节流)
          2. getcode(): 返回服务器响应码
            print(res.getcode())
            200: 成功
            4xx: 服务器页面出错
            5xx: 服务器出错
          3. geturl(): 返回实际数据的URL(防止重定向问题)
      3. urllib.parse模块
        1. urllib.parse.urlencode({字典})
          编码前: {"wd": "帅哥"}
          编码后: "wd=%e8%a5"
          注意:
            1. urlencode的参数一定要为 字典, 得到的结果为字符串
            2. with open("baidu.html", "w", encoding="utf-8") as f
        2. urllib.parse.quote("字符串")
          1. http://www.baidu.com/s?wd=帅哥
          2. key = urllib.parse.quote("帅哥")
            key的值: "%e8%a3..."
        3. urllib.parse.unquote("字符串")
          s = "%d3%e8..."
          s = urllib.parse.unquote(s)
        4. 练习
          百度贴吧数据抓取
          要求:
            1. 输入要抓取的贴吧名称:
            2. 输入起始页和终止页
            3. 把一页保存到本地
              第1页.html,第2页.html
          步骤:
            1. 找URL规律
            2. 获取网页内容(发请求, 获响应)
            3. 保存(本地 / 数据库)

### 请求方式及案例
    1. GET
      1. 特点: 查询参数在URL地址上显示
      2. 案例: 抓取百度贴吧
    2. POST(在Request方法中添加一个data参数)
      1. req = urllib.request.Request(url, data=data, headers=headers)
        data: 表单数据以bytes类型提交, 不能是string
      2. 案例: 有道翻译
        1. 要求
          用户输入需要翻译的内容, 把翻译的结果给输出来
      3. 如何把json格式的字符串转换为Python中字典
        import json
        s = "{'key': 'value'}"
        s_dict = json.loads(s)

### nginx
    1. 概念
      1. Django项目(Web应用)
      2. Web服务
        1. Nginx: 高并发处理的好
        2. Apache: 更稳定
          LAMP: Linux, Apache, MySQL, Python/Php/Perl
          LNMP: Linux, Nginx, MySQL, Python/Php/Perl
        3. uwsgi
          是Web服务器与Web框架之间一种简单而通用的接口
    2. 项目部署(nginx+uwsgi+django)
      1. 确保Django项目能够运行
      2. 安装Nginx
        1. 安装: sudo apt-get install nginx
        2. 启动: sudo /etc/init.d/nginx restart
        3. 验证: 打开浏览器, 输入127.0.0.1:80
          Welcome to nginx
      3. 安装uwsgi(用pip3)
        1. 安装: sudo pip3 install uwsgi
        2. 验证: uwsgi --http :9998 --chdir /home/tarena/aid1807/Django/FruitDay/ --module FruitDay.wsgi
      4. 部署
        1. 配置uwsgi(配置文件)
          1. mkdir uwsgi
          2. cd uwsgi
          3. touch FruitDay_uwsgi.ini
            [uwsgi]
            #: 指定和nginx通信的端口
            socket=127.0.0.1:8001
            # 项目路径
            chdir=/home/tar/home/tarena/aid1807/Django/FruitDay/
            # wsgi.py路径
            wsgi-file=FruitDay.wsgi.py
            # 进程数
            processes=4
            # 线程数
            thread=2
            # 本项目占用uwsgi的端口
            stats=127.0.0.1:8081
        2. 配置nginx(配置文件)
          1. sudo -i
          2. cd /etc/nginx/sites-enabled
          3. vi project_nginx.conf
          4. 重启nginx服务
            sudo /etc/init.d/nginx restart
          5. 拷贝uwsgi_params文件到项目路径下
            sudo cp /etc/nginx/uwsgi_params /home/tarena/aid1807/Django/FruitDay/
        3. 收集静态文件
          1. 在settings.py中添加路径(STATIC_ROOT)
            '/home/tarena/aid1807/Django/FruitDay/static'
          2. 收集静态文件
            ./manage.py collectstatic
        4. uwsgi启动项目
          cd /home/tarena/uwsgi
          uwsgi --ini FruitDay_uwsgi.ini
    3. 多项目部署
      1. uwsgi
        每个项目都需要单独创建uwsgi配置文件, 选用不同的端口
      2. nginx
        更改配置文件, 1个就可以, 添加server{}
      3. 部署个人博客项目
        1. 浏览器访问端口: 8202
          vi /etc/nginx/sites-enabled/protect_nginx.conf
          server{
            listen 8202
            ... ...
          }
        2. uwsgi和nginx互相通信的端口: 8002
          1. 修改uwsgi
            cd uwsgi
            vi blog_uwsgi.ini
            [uwsgi]
            socket=127.0.0.1:8002
          2. 修改nginx配置文件
            server{
              ...
              ...
              location /{
                include uwsgi_params;
                uwsgi_pass 127.0.0.1:8002
              }
            }
        3. uwsgi启动个人博客项目占用的端口号: 8082
          blog_uwsgi.ini
          [uwsgi]
          ...
          stats=127.0.0.1:8082
